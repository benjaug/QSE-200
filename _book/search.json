[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "QSE 200 Sections",
    "section": "",
    "text": "This book contains the notes for recitation sections covered in QSE 200 during Fall 2022. This course was a first-year graduate-level treatment of quantum mechanics with a special emphasis on numerical problems in quantum mechanics."
  },
  {
    "objectID": "Section2.html",
    "href": "Section2.html",
    "title": "1  Section 2: Numerically Solving the TDSE",
    "section": "",
    "text": "Last week, we developed code to solve the time-independent Schrodinger equation (TISE). In that code, we used a second-order finite difference approximation to the second derivative and then computed numerical eigenvalues and eigenvectors associated with the Hamiltonian. In this week’s section, we’ll take a similar approach to solve the time-dependent Schrodinger equation (TDSE). After developing the numerical method and writing code to implement it, we will propagate Gaussian wavepackets to look at their behavior in free space and as they interact with some interesting potentials.\nLearning Goals: After this Section you should be able to: - Understand how to discretize the Schrodinger equation in both space and time - Explain the difference between the Crank-Nicolson method and the forward- or backward-difference methods - Compute and describe the behavior of a Gaussian wavepacket moving in free space - Simulate and qualitatively rationalize the behavior of a Gaussian wavepacket interacting with at least one potential"
  },
  {
    "objectID": "Section2.html#background-simulating-time-dependence",
    "href": "Section2.html#background-simulating-time-dependence",
    "title": "1  Section 2: Numerically Solving the TDSE",
    "section": "1.1 Background: Simulating Time Dependence",
    "text": "1.1 Background: Simulating Time Dependence\nIn class, we talked about one way to think about how an arbitrary wavefunction evolves in time that relies on the simple time evolution of energy eigenstates. That idea was as follows: - Express an arbitrary wavefunction \\(\\phi(x)\\) as a superposition of the energy eigenstates for the specific potential of interest. This gives you an expression like \\(\\phi(x) = \\sum_{n} c_n \\psi_n(x)\\), where the \\(c_n\\) can be determined (either numerically or analytically) by taking inner products between the state \\(\\phi(x)\\) and the energy eigenstates. - Append the appropriate time-dependent phase factor to each term in this series, yielding \\(\\phi(x,t) = \\sum_{n} c_n \\psi_n(x) e^{-i \\omega_n t}\\). - Evaluate the sum at each time \\(t\\) of interest.\nYou’ll get more practice with this idea on future homeworks and in future Sections, but today we are going to talk about a different way to find time-dependent solutions to the Schrodinger equation—specifically, we’ll try to integrate the Schrodinger equation directly without relying on a prior determination of the energy eigenstates. We are doing this so you can get more exposure to numerical solutions of differential equations and learn some intuition about how free particles move through different potentials.\nThe specific method we will use is called the Crank-Nicolson method. It is probably one of the simplest possible ways to solve the TDSE; it is not particularly efficient, but it is good enough for our purposes right now."
  },
  {
    "objectID": "Section2.html#deriving-the-crank-nicolson-method",
    "href": "Section2.html#deriving-the-crank-nicolson-method",
    "title": "1  Section 2: Numerically Solving the TDSE",
    "section": "1.2 Deriving the Crank-Nicolson Method",
    "text": "1.2 Deriving the Crank-Nicolson Method\nRecall from class that the TDSE is given by\n\\[\ni \\hbar \\frac{\\partial \\Psi}{\\partial t} = -\\frac{\\hbar^2}{2m} \\frac{\\partial^2 \\Psi}{\\partial^2 x} + V(x) \\Psi.\n\\tag{1.1}\\]\nThe right-hand side is exactly the same as the Hamiltonian terms involved in the TISE, but now we also need to handle the temporal derivative.\nLet’s use \\(j\\) as a subscript index to discretize space, with step size \\(\\Delta x\\), and \\(n\\) as a superscript index to discretize time, with step size \\(\\Delta t\\).\nQuestion for the class: How should we fill in the following equations? \\[\n\\frac{\\partial^2 \\Psi}{\\partial^2 x} \\rightarrow \\frac{\\Psi_{j+1} - 2 \\Psi_{j} + \\Psi_{j-1}}{\\Delta x^2}\n\\tag{1.2}\\]\n\\[\n\\frac{\\partial \\Psi}{\\partial t} \\rightarrow \\frac{\\Psi^{n} - \\Psi^{n-1}}{\\Delta t}\n\\tag{1.3}\\]\nOnce we have the discretization formulas, we can plug these back into Equation 1.1. But here’s a question: at which time index do we evaluate the right-hand side?\n\n1.2.1 Forward-Difference Approximation\nThe obvious choise might be to use time \\(n-1\\) on the RHS, since then we have an equation that gives \\(\\Psi_{j}^{n}\\) in terms of \\(\\Psi_{j-1}^{n-1}\\), \\(\\Psi_{j}^{n-1}\\), and \\(\\Psi_{j+1}^{n-1}\\). However, this has a problem! It introduces errors that can grow exponentially in time. That is NOT good.\nQuestion for the class: Can you see how you might convince yourself this is true?\n\n\n1.2.2 Backward-Difference Approximation\nOk, so we can’t use the simplest solution and still obtain a stable method. What if we express \\(\\Psi_{j}^{n}\\) in terms of \\(\\Psi^{n+1}?\\) That’s a little funny because now the equation relates \\(\\Psi_{j}^{n}\\) to a bunch of values that haven’t been determined yet! It’s actually not a huge deal because we can in principle solve a system of (linear) equations to obtain the wavefunctions. But this method actually doesn’t conserve probability. That is also NOT good.\nQuestion for the class: Can you see how to convince yourself about the issues with the backward-difference method? (This one is harder at this stage in the course… but (if this makes sense to you from your previous QM knowledge) you should think about the idea of unitary evolution."
  },
  {
    "objectID": "Section2.html#crank-nicolson-method",
    "href": "Section2.html#crank-nicolson-method",
    "title": "1  Section 2: Numerically Solving the TDSE",
    "section": "1.3 Crank-Nicolson Method",
    "text": "1.3 Crank-Nicolson Method\nIt turns out, we can play a clever trick developed by John Crank and Phyllis Nicolson. We can average the forward- and backward-difference approximations together. It turns out this solves the problems associated with stability and probability conservation, and it’s pretty easy to code. In component form, this looks like:\n\\[\ni \\hbar \\frac{\\Psi_{j}^{n} - \\Psi_{j}^{n-1}}{\\Delta t} = -\\frac{\\hbar^2}{2m} \\frac{(\\Psi_{j+1}^{n} - 2 \\Psi_{j}^{n} + \\Psi_{j-1}^{n}) + (\\Psi_{j+1}^{n-1} - 2 \\Psi_{j}^{n-1} + \\Psi_{j-1}^{n-1})}{2 \\Delta x^2} + V_j \\frac{\\Psi_{j}^{n} + \\Psi_{j}^{n-1}}{2}.\n\\tag{1.4}\\]\nIf we sort the terms by their time index (index \\(n\\) on the LHS and index \\(n-1\\) on the RHS), we get: \\[\n(1+ i \\Delta t H/2\\hbar) \\Psi^{n} = (1-i \\Delta t H/2\\hbar) \\Psi^{n-1}\n\\tag{1.5}\\]\nThis is great. It means that if we build a matrix representation of the Hamiltonian (which we already know how to do!) we can propagate the wavefunction in time by multiplying by \\[\nC = (1+i \\Delta t H/2 \\hbar)^{-1} (1-i \\Delta t H/2\\hbar),\n\\tag{1.6}\\] where \\(C\\) is the Crank-Nicolson matrix that advances by one time step: \\[\n\\Psi^{n} = C \\Psi^{n-1}.\n\\tag{1.7}\\] So long as \\(H\\) itself doesn’t depend on time, we only need to compute the matrix \\(C\\) once, which makes the algorithm more efficient."
  },
  {
    "objectID": "Section2.html#task-1-write-functions-to-propagate-a-gaussian-wavepacket",
    "href": "Section2.html#task-1-write-functions-to-propagate-a-gaussian-wavepacket",
    "title": "1  Section 2: Numerically Solving the TDSE",
    "section": "1.4 Task 1: Write functions to propagate a Gaussian wavepacket",
    "text": "1.4 Task 1: Write functions to propagate a Gaussian wavepacket\nNow let’s get to the code. First of all, here are the numerical libraries that will be most useful for you today. If you want to add more, just import them in the box below.\n\n# Import the typical numerical and plotting libraries\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n%matplotlib inline\n\nGet started by writing a function make_gaussian that creates a Gaussian wavepacket. We wrote the function signature for you, but you’ll have to write the body. Make sure that it outputs a vector containing the amplitude of the Gaussian wavepacket at each point in your x grid.\n\ndef make_gaussian(x, x0, sigmax, k0):\n    Norm_psi = 1/np.sqrt(sigmax*np.sqrt(2*np.pi))\n    psi = Norm_psi * np.exp(1j*k0*x) * np.exp(-(x-x0)**2/(2*sigmax)**2)\n    return psi\n\nMake sure your code works by generating and plotting a Gaussian wavepacket.\n\n%matplotlib inline\nxtest = np.linspace(-10,10)\ntest_gaussian = make_gaussian(xtest, 0, 1,10)\nplt.plot(xtest, test_gaussian)\nplt.xlabel('x')\nplt.ylabel('Norm-Squared')\n\nText(0, 0.5, 'Norm-Squared')\n\n\n\n\n\nNow write the functions to make your Gaussian wavepacket move. In the block below, fill in the functions make_H to build the Hamiltonian and return it as a matrix H and make_C to make the Crank-Nicolson propagation matrix as a matrix CN. We need to use Equation 1.6 and Equation 1.7 to do this.\n\n# Function to make the Hamiltonian given V (in vector form)\ndef make_H(V):\n    N = np.size(V)\n    y = -hbar**2/(2*m*dx**2) # helper variable\n    H = np.zeros((N,N))\n    d0 = np.ones(N) * (-2*y + V) # this is the main diagonal\n    d1 = np.ones(N-1) * (y) # This is the +1/-1 diagonal\n    H = np.diag(d0) + np.diag(d1, 1) + np.diag(d1, -1) # put it together\n    H[0,N-1] = 0 # I'll use periodic boundary conditions but it shouldn't really matter... \n    H[N-1,0] = 0\n    return H\n\n# Turn the Hamiltonian into the Crank-Nicolson matrix.\ndef make_C(V):\n    H = make_H(V)\n    N = H.shape[1] # Get Hamiltonian dimensions\n    z = 0.5*1j*dt/hbar\n    CN = np.linalg.inv(np.eye(N) + z*H) @ (np.eye(N) - z*H)\n    return CN\n\nThe last thing to do before running simulations is to implement the time-stepping. Complete the function simulate which, using your other functions, builds a normalized Gaussian wavepacket and propagates it in time. You want this function to return a list of the wavefunctions computed at each time step. (Notice how in the function signature we have used default values, which is a useful trick you can use in your own code when there are certain values that you might not want to keep resetting over and over…)\n\n# Function to run the simulation\ndef simulate(V, psi0, max_t_step = 350):\n    psi=psi0\n    # Make the C-N matrices\n    CNmat = make_C(V)\n\n    # run simulation\n    tlist = dt * np.arange(max_t_step)\n    psi_list = np.zeros((tlist.shape[0], psi.shape[0]), dtype=complex)\n    for i in range(tlist.shape[0]):\n        psi = CNmat @ psi\n        psi_list[i,:] = psi  \n        \n    return psi_list\n\nBefore moving on, let’s run a simple test of your code to make sure things seem to work. We’ll do this by simulating the motion of a free particle. Do this by running your simulate code on the potential energy function \\(V(x) = 0\\) and watching your particle propagate in space. Is the behavior qualitative correct?\nFirst of all, set up your simulation. We are going to work in a “natural” set of units that are defined for you already below.\n\n# Parameters set for you.\nhbar = 1 # hbar is 1 here.\nm = 0.5 # electron mass\n\n# These are parameters you can play with to adjust the simulation\nx0 = -60 # initial position\nk0 = np.pi/10 * 2 # Initial average wavenumber NOTE: Max k value is 2*pi/(2*dx)\nsigmak = 1/12 * 1/4 # Initial width in k-space\nsimWidth = 1600 # Width of simulation\ndt = 1 # Time step\ndx = 1 # Grid step\n\n# ... and these parameters are fixed by the values you set above.\nN = simWidth + 1        # Number of grid points\nsigmax = 1/(2*sigmak)   # Initial width in position space\nwindowWidth = simWidth*dx  # Plot window size\ndp = 2*np.pi*hbar/(N*dx)   # k-space width of grid point\nE0 = hbar**2*k0**2/(2*m)  # Average energy of initial wave packet\nvelocity0 = np.sqrt(2*E0/m)   # Average velocity of initial wave packet\n\nYou should define a position-space grid with \\(x\\) values ranging from \\(x_0\\) to \\(x_N\\). Make sure to center your well at \\(x=0\\) in the sense that it extends from -simWidth/2 to simWidth/2. Do that here:\n\nx = dx * (np.arange(0,N-1) - simWidth/2)\n\nNow, make a Gaussian wavefunction evolving in free space (\\(V=0\\) everywhere) and with no initial momentum. Plot it and watch it evolve in time from \\(t=0\\) to \\(t=300\\) (in our dimensionless units). What does the wavefunction do?\n\ntest_gaussian = make_gaussian(x, 0, 10, 0.0)\nVs = np.zeros(test_gaussian.shape)\npsi_out = simulate(Vs, test_gaussian)\n\n%matplotlib inline\nfor i in range(psi_out.shape[0]):\n    plt.plot(x, np.abs(psi_out[i,:]))\nplt.xlim(-200,200)\n\n\n(-200.0, 200.0)\n\n\n\n\n\nRepeat the question above, but now give the Gaussian wavepacket an initial momentum to the right by setting \\(k_0 \\neq 0\\). Now how does the wavefunction evolve?\n\ntest_gaussian = make_gaussian(x,x0,sigmax,k0)\nVs = np.zeros(test_gaussian.shape)\npsi_out = simulate(0*Vs, test_gaussian, 300)\n\nfor i in range(300):\n    plt.plot(x, np.abs(psi_out[i,:])**2)\nplt.xlim(-300,300)\n\n(-300.0, 300.0)"
  },
  {
    "objectID": "Section2.html#an-aside-on-animations",
    "href": "Section2.html#an-aside-on-animations",
    "title": "1  Section 2: Numerically Solving the TDSE",
    "section": "1.5 An aside on animations",
    "text": "1.5 An aside on animations\nAnimating your plots in Python takes a little bit of work, so we have written a little function for you that generates an animation that can be plotted inside the Jupyter notebook. You can use this function by feeding it a list of wavefunctions and (optionally) plot limits. The function will automatically plot the probability density associated with your wavefunction list. (You’ll need to modify it if you want to see the wavefunction directly…)\n\ndef make_animation(psi_list, xmin=-100, xmax=100, ymin=0, ymax=0.05, frame_interval=5):\n    %matplotlib notebook\n    fig, ax = plt.subplots()\n    line, = ax.plot([])     # A tuple unpacking to unpack the only plot\n    ax.set_xlim(xmin,xmax)\n    ax.set_ylim(ymin,ymax)\n\n    def animate(frame_num):\n        y = np.abs(psi_list[frame_num,:])**2\n        line.set_data((x, y))\n        return line\n\n    anim = FuncAnimation(fig, animate, frames=np.shape(psi_list)[0], interval=frame_interval)\n    return anim\n\nAs an example of how to use this, let’s plot the free particle moving through space:\n\ntempvar = make_animation(psi_out, frame_interval=20)\ntempvar\nplt.show()"
  },
  {
    "objectID": "Section2.html#project-1-simulating-a-quantum-mass-on-a-spring",
    "href": "Section2.html#project-1-simulating-a-quantum-mass-on-a-spring",
    "title": "1  Section 2: Numerically Solving the TDSE",
    "section": "2.1 Project 1: Simulating a quantum mass on a spring",
    "text": "2.1 Project 1: Simulating a quantum mass on a spring\nThis section is set up to simulate a Gaussian wavepacket evolving in a harmonic potential, \\(V(x) = \\frac{1}{2} \\omega^2 x^2\\). In the classical case, we know that this gives rise to simple harmonic motion with \\(x(t) \\sim cos(\\omega t + \\varphi)\\). Here you will write code to see if the same thing happens in the quantum world, or to what extent things differ…\nAnd you’ll also need to define a potential \\(V(x)\\) for the particle to evolve in. Define that by filling in the code block below. To pick a reasonable value for \\(\\omega\\), remember that the value of \\(E_0\\) is tied to the value of \\(k_0\\) and we can’t simulate momenta with values greater than \\(\\sim 2\\pi/2\\Delta x\\). Try to think about what that means for how you should set \\(\\omega\\) such that it is compatible with your previously chosen simulation parameters…\n\n# set up potential with default values\ndef Vfunc(a=15, V0=1.5):\n    return 1/2 * 30 * E0*(x/np.max(x))**2\n\nVs = Vfunc(15,1.5)\n\n\nplt.plot(x,Vs)\n\n\ntest_gaussian = make_gaussian(x,x0,sigmax,0)\ndone=simulate(Vs, test_gaussian, 800)\n\n\naaa=make_animation(done, xmin=-100, xmax=100, ymin=0, ymax=0.09)\nplt.show()\n\n\n\n\n\n\n\nBelow, write some code to compute the expectation value of \\(x\\) as a function of time and plot it. Does it evolve like you would expect for a classical particle? If it’s different in any way, why do you think it behaves this way?\n\nxexp = np.zeros(done.shape[0])\nfor n in range(done.shape[0]):\n    xexp[n] = np.sum(x*np.abs(done[n,:])**2)\n    \n%matplotlib inline\nplt.plot(xexp)\n\n\n\n\nOnce you have a feeling for how the particle evolves, try changing the potential, for example looking at a quartic (\\(\\propto x^4\\)) potential. How does the evolution of a particle in this potential differ from that of a harmonic potential?"
  },
  {
    "objectID": "Section2.html#option-2-scattering-from-square-and-sech-squared-potential-wells",
    "href": "Section2.html#option-2-scattering-from-square-and-sech-squared-potential-wells",
    "title": "1  Section 2: Numerically Solving the TDSE",
    "section": "2.2 Option 2: Scattering from Square and Sech-Squared Potential Wells",
    "text": "2.2 Option 2: Scattering from Square and Sech-Squared Potential Wells\nAn unusual aspect of quantum mechanics is that there can be reflection of a particle even from a potential well. Here, you will simulate this effect by comparing the scattering of a particle crashing into a square barrier (with positive energy) and a square well (with negative energy). You’ll also compare your findings to a particularly interesting potential well, \\(V(x) \\sim - \\text{sech}(x)^2\\) and uncover an unusual property of this type of well.\nFirst of all, write some code that defines a square well and the sech-squared well. Note that numpy doesn’t have the sech function implemented, but it does have a cosh function.\nTo choose your well depths, pick something that has a width of \\(\\sim 5a - 50a\\) and a depth (or height) that is a couple units of \\(E_0\\). Generate vectors V below for the different potentials you want to simulate. Plot them with a y-axis normalized by \\(E_0\\) to make sure they are a reasonable scale. (And think about why we need to choose this carefully—hint, think about the simulation parameters we have set.)\n\n# set up potential with default values\ndef Vfunc(a=15, V0=1):\n    return -1.15 * (np.abs(x) < a)\n\nV_sq = Vfunc(8,6)\n\ndef Vfunc(a=15, V0=1):\n    return -(hbar**2*V0*(V0+1))/(2*m*a**2)*1/np.cosh(x/a)**2\n\nV_sech = Vfunc(5,5.5)\n\n\nplt.plot(x,V_sq/E0)\nplt.plot(x,V_sech/E0)\nplt.xlim((-100,100))\n\n(-100.0, 100.0)\n\n\n\n\n\nNow write code to propagate a Gaussian wavepacket through your different potentials.\n\ntest_gaussian = make_gaussian(x,3*x0,sigmax,k0)\n\ndef Vfunc(a=15, V0=1):\n    return -(hbar**2*V0*(V0+1))/(2*m*a**2)*1/np.cosh(x/a)**2\n\nVsech = Vfunc(7,8.75)\npsi_out_sech = simulate(Vsech, test_gaussian, 800)\n\ndef Vfunc(a=15, V0=1):\n    return -1.15 * (np.abs(x) < a)\n\nVsq = Vfunc(5,5)\npsi_out_sq = simulate(Vsq, test_gaussian, 800)\n\nAnimate your solutions and describe the behavior of a particle that encounters a square well. What happens in this case?\n\naaa=make_animation(psi_out_sq, xmin=-300, xmax=300, ymin=0, ymax=0.03)\nplt.show()\n\n\n\n\n\n\n\n/opt/conda/lib/python3.10/site-packages/matplotlib/animation.py:887: UserWarning: Animation was deleted without rendering anything. This is most likely not intended. To prevent deletion, assign the Animation to a variable, e.g. `anim`, that exists until you have outputted the Animation using `plt.show()` or `anim.save()`.\n  warnings.warn(\n\n\nThen, simulate a particle encountering a sech-squared well and describe the differences. What is the major change in this case? Why do you think the behavior is so different? (Hint: An analogy to optics may be useful.) Why might a potential with that sort of behavior be useful?\n\naaa=make_animation(psi_out_sech, xmin=-300, xmax=300, ymin=0, ymax=0.03)\nplt.show()"
  },
  {
    "objectID": "Section4.html",
    "href": "Section4.html",
    "title": "2  Section 4: Discrete Variable Representation",
    "section": "",
    "text": "Last week, we studied the connection between position and momentum space using Fourier transforms. We learned that we can work in either “position space” with basis states \\(\\langle x' | x \\rangle = \\delta(x-x')\\), or in “momentum space” with basis states \\(\\langle k' | k \\rangle = \\delta(k-k')\\). We also showed that these kets are connected by the relation \\[\\begin{equation}\n\\langle k | x \\rangle = (2\\pi)^{-1/2} e^{-i k x}.\n\\end{equation}\\] In this week’s section, we will use what we learned to build and solve matrix representations of the Hamiltonian for arbitrary 1D potentials using a method called the “Discerete Variable Representation” (DVR). This is a method that is widely used in physics and chemistry to solve for the states and energies of potential energy surfaces needed to describe molecular vibrations, chemical reactions, and beyond.\nLearning Goals: After this Section you should be able to: - Understand when it is more natural to work in a “position” vs. “momentum” basis - Use the sinc basis to express the Hamiltonian for an arbitrary 1D potential - Find the eigenvalues and eigenvectors of Hamiltonians for arbtirary potentials using the discrete variable representation"
  },
  {
    "objectID": "Section4.html#motivating-the-sinc-basis-dvr",
    "href": "Section4.html#motivating-the-sinc-basis-dvr",
    "title": "2  Section 4: Discrete Variable Representation",
    "section": "2.1 Motivating the Sinc-Basis DVR",
    "text": "2.1 Motivating the Sinc-Basis DVR\nAs with previous problems, we’ll first introduce a grid of \\(N\\) equally spaced points on the domain \\([x_\\text{min}, x_\\text{max}]\\). The points can be specified as \\[\\begin{equation}\nx_j = x_\\text{min} + (j-1) \\Delta x\n\\end{equation}\\] where \\[\\begin{equation}\n\\Delta x = \\frac{x_\\text{max} - x_\\text{min}}{N-1}.\n\\end{equation}\\]\nNext, we want to set up some basis \\(\\{|\\phi_i\\rangle\\}\\) that allows us to express our wavefunction as a superposition of convenient basis functions. Putting this into math based on last week’s section, we want a way to implement the expansion \\[\\begin{equation}\n\\psi(x) = \\langle x | \\psi \\rangle = \\sum_{i=1}^{n} c_i \\langle x | \\phi_i \\rangle = \\sum_{i=1}^{n} c_i \\phi_i(x).\n\\end{equation}\\]\nThere are many possible reasonable choices of this basis, and there is a vast literature of DVR methods that use different choices (some that can’t even be written as nice analytical functions!). But let’s think about what we would want in a general DVR basis set.\nQuestion: What nice properties might we want?\nAnswer: Some example of useful properties would be: - We want the basis to satisfy \\(\\langle x_i | x_j \\rangle \\propto \\delta_{i,j}\\) - It would be nice if the basis functions were “well behaved” so that we can evaluate derivatives, Fourier transforms, etc.\nOne example of a set of functions that satisfy these desiderata are the sinc functions: \\[\\begin{equation}\n\\phi_j(x) = \\frac{1}{\\sqrt{\\Delta x}} \\frac{\\sin[\\pi (x-x_j)/\\Delta x]}{\\pi (x-x_j)/\\Delta x} = \\frac{1}{\\sqrt{\\Delta x}} \\text{sinc} [\\pi (x-x_j)/\\Delta x].\n\\end{equation}\\]\nFirst of all, let’s explore why these basis functions are useful. It’s best to start this discussion graphically, so let’s use some code to implement the basis and plot the basis functions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nIn the space below, we give you some code that defines a grid of x points and a function that implements the sinc-basis. Use these pieces of code to explore the sinc-basis functions and learn about their properties. You are looking to answer questions like the following: - What is the value of a basis function \\(\\phi_j\\) at grid point \\(x_j\\)? - What is the value of a basis function \\(\\phi_j\\) at a grid point \\(x_{j+1}\\)? At \\(x_{j+2}\\)? - In general, how do the basis functions relate to one another?\n\n# Parameters that we set\nxmin = 0\nxmax = 9\nN = 10\n\n# Grid spacing and position-space grid points\nDelta_x = (xmax-xmin)/(N-1)\nxs = np.linspace(xmin,xmax,N)\nxs_fine = np.arange(xmin,xmax,Delta_x/1000)\n\n# Definition of the sinc-basis functions\ndef phi(x,xj):\n    return 1/np.sqrt(Delta_x) * np.sinc((x-xj)/Delta_x)\n\nThere are two useful ways that we can look at these basis functions. First of all, we can plot an example like \\(\\phi_3(x_j)\\) to look at the \\(i=3\\) basis function evaluated at each grid point. If we overlay this with a plot of \\(\\phi_3(x)\\) evaluated on a much finer grid, we see that the sinc-basis is defined such that a given basis function \\(\\phi_i\\) is nonzero at grid point \\(x_i\\), and its oscillations ensure that it passes through zero at every other grid point!\nQuestion: Does it matter that the sinc function takes on nonzero values away from the \\(i\\neq j\\) grid points?\n\nplt.axhline(0,color='k')\nplt.plot(xs_fine, phi(xs_fine, xs[3]), label=f\"$\\phi_3(x)$\")\nplt.plot(xs,phi(xs,xs[3]),'ro', label=f\"$\\phi_3(x_j)$\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid()\n\n\n\n\nIt can also be useful to look at a series of these sinc functions centered at different grid points. Notice where they all pass through zero, and where each individual basis function doesn’t pass through zero.\n\nplt.axhline(0,color='k')\nfor j in range(3,9):\n    plt.plot(xs_fine, phi(xs_fine, xs[j]),label=f\"i={j}\")\n    \nplt.legend()\nplt.grid()\n\n\n\n\n\n2.1.1 Interpreting the Sinc-Basis\nQuestion: In light of these plots, how can we interpret the sinc-basis functions as compared to a set of Dirac delta functions?\nAnswer: This is a finite resolution basis that is like the closest possible approximation to a delta function that we can make for a given range on the number of momentum basis functions that our spatial grid can accomodate. As the plots above show, we means this in the sense that these basis states satisfy \\[\\begin{equation}\n\\phi_j(x_k) \\propto \\delta_{k,j},\n\\end{equation}\\] so a given basis function only gives a nonzero contribution to a function at the point at which that basis function is centered.\nThe sinc basis has a nice relationship to the Fourier transforms that we discussed last week. In particular, the sinc basis functions can be written as follows: \\[\\begin{equation}\n\\phi_j(x) = \\frac{\\sqrt{\\Delta x}}{2\\pi} \\int_{-\\pi/\\Delta x}^{\\pi/\\Delta x} e^{i k(x-x_j)} dk\n\\end{equation}\\]\nQuestion: Can you justify this form qualitatively? How does it relate to Fourier transforms that we studied last week? And why are the limits on the integral those numbers?"
  },
  {
    "objectID": "Section4.html#deriving-the-kinetic-energy-matrix",
    "href": "Section4.html#deriving-the-kinetic-energy-matrix",
    "title": "2  Section 4: Discrete Variable Representation",
    "section": "2.2 Deriving the Kinetic Energy Matrix",
    "text": "2.2 Deriving the Kinetic Energy Matrix\nNow that we have a useful basis, we can try to solve problems with it. We still need to learn how to express the Hamiltonian in this basis. Since the Hamiltonian is \\[\\begin{equation}\nH = -\\frac{\\hbar^2}{2m} \\frac{d^2}{dx^2} + V(x)\n\\end{equation}\\] we need to figure out how to express the second derivative operator and the \\(V(x)\\) operator in our chosen basis.\nLet’s start with the kinetic energy term. We want to derive a kinetic energy matrix with matrix elements \\[\\begin{equation}\nT_{ij} = -\\frac{\\hbar^2}{2m} \\left \\langle \\phi_i \\left\\lvert \\frac{d^2}{dx^2} \\right\\rvert \\phi_j \\right \\rangle.\n\\end{equation}\\]\nIt turns out this is where the Fourier transform expression is super helpful. We can go through the following steps: \\[\\begin{equation}\n\\int_{-\\infty}^{\\infty} \\phi_i^\\ast(x) \\frac{d^2}{dx^2} \\phi_j(x) dx = \\frac{\\Delta x}{(2\\pi)^2} \\int_{-\\infty}^{\\infty} dx \\int_{-\\pi/\\Delta x}^{\\pi/\\Delta x} e^{-i k (x-x_i)} dk \\int_{-\\pi/\\Delta x}^{\\pi/\\Delta x} \\left( \\frac{d^2}{dx^2} e^{i k' (x-x_j)} \\right) dk'\n\\end{equation}\\]\nBecause we’ve expressed the basis function in terms of plane waves, evaluating \\(\\frac{d^2}{dx^2}\\) is really easy: it just corresponds to pulling down a factor of \\((ik)^2\\). That gets us to \\[\\begin{equation}\n\\frac{\\Delta x}{(2\\pi)^2} \\int_{-\\pi/\\Delta x}^{\\pi/\\Delta x} (ik)^2 e^{i k (x_i - x_j)} dk.\n\\end{equation}\\]\nIf you work through these integrals, you will find that: \\[\\begin{equation}\n\\boxed{T_{i,j=i} = \\frac{\\hbar^2}{2m \\Delta x^2} \\frac{\\pi^2}{3}}\n\\end{equation}\\] and \\[\\begin{equation}\n\\boxed{T_{i,j \\neq i} = \\frac{\\hbar^2}{2m \\Delta x^2} \\frac{2 (-1)^{i-j}}{(i-j)^2}}\n\\end{equation}\\]\nTask: Now fill in the code below to make a function that builds the kinetic energy operator and the potential energy operator.\n\n# work in some convenient units\nhbar = 1\nm = 1\n\n\n# Function to make the kinetic energy operator\ndef make_T(x):\n    Delta_x = x[1]-x[0]\n    N = xs.shape[0]\n    Tmat = np.zeros((N,N))\n    \n    # now loop over kinetic energy matrix and fill in matrix elements\n    for i in range(N):\n        for j in range(N):\n            if i==j:\n                Tmat[i,j] = (hbar**2/(2*m*Delta_x**2)) * (-1)**(i-j) * (np.pi**2)/3\n            else:\n                Tmat[i,j] = (hbar**2/(2*m*Delta_x**2)) * (-1)**(i-j) * 2/(i-j)**2\n                \n    return Tmat"
  },
  {
    "objectID": "Section4.html#deriving-the-potential-energy-matrix",
    "href": "Section4.html#deriving-the-potential-energy-matrix",
    "title": "2  Section 4: Discrete Variable Representation",
    "section": "2.3 Deriving the Potential Energy Matrix",
    "text": "2.3 Deriving the Potential Energy Matrix\nFor the potential energy matrix, we need to express \\(V(x)\\) in matrix form. Since \\(V(x)\\) can be expanded as a power series in the position operator \\(\\hat{x}\\), let’s first look at the matrix form of \\(\\hat{x}\\) itself. We want the matrix elements \\(x_{ij} = \\langle \\phi_i | x | \\phi_j \\rangle\\). In other words, we want \\[\\begin{equation}\n\\int_{-\\infty}^{\\infty} \\phi_i(x)^\\ast x \\phi_j(x) dx = \\frac{\\Delta x}{(2\\pi)^2} \\int_{-\\pi/\\Delta x}^{\\pi/\\Delta x} dk \\int_{-\\pi/\\Delta x}^{\\pi/\\Delta x} dk' e^{ikx_i - ik'x_j} \\int_{-\\infty}^{\\infty} x e^{i x (k'-k)} dx.\n\\end{equation}\\] In the last term, note that we can replace \\(x\\) by \\(\\frac{d}{d(ik')}\\) and then pull the derivative outside of the integral over \\(x\\) (since it doesn’t depend on \\(x\\) anymore). Working through the integrals after that step, we get to \\[\\begin{equation}\nx_{ij} = \\frac{\\Delta x}{2\\pi} x_i \\int_{-\\pi/\\Delta x}{\\pi/\\Delta x} dk' e^{ik'(x_i-x_j)} = x_i \\delta_{i,j}.\n\\end{equation}\\]\nThe important conclusion is that \\(\\hat{x}\\) is diagonal in this basis! So the potential energy, which can be expanded in powers of \\(x\\) is also diagonal in this basis! Knowing this, we can just make the potential energy operator by building a diagonal matrix from \\(V(x)\\). The function make_V does this, and then the function make_H just calls the kinetic and potential energy functions and puts them together.\n\n# Function to make the potential energy operator\ndef make_V(x,Vfunc):\n    Vmat = np.diag(Vfunc(x))\n    return Vmat\n\n# Function to make the full Hamiltonian\ndef make_H(x,Vfunc):\n    return make_T(x) + make_V(x,Vfunc)"
  },
  {
    "objectID": "Section4.html#testing-the-dvr-code",
    "href": "Section4.html#testing-the-dvr-code",
    "title": "2  Section 4: Discrete Variable Representation",
    "section": "2.4 Testing the DVR code",
    "text": "2.4 Testing the DVR code\nThat was a lot of work to get our Hamiltonian matrix. But now let’s see the payoff! We’ll see that the DVR code is incredibly accurate, especially compared to the finite differences method that we used before. As always let’s test our code by using a harmonic oscillator potential. Let’s start with a 20 point grid and see how the calculations do for energies and wavefunctions.\n\nxs = np.linspace(-10,10,20)\n\n\ndef Vharmonic(x):\n    return 0.5*x**2\n\n\nHam = make_H(xs,Vharmonic)\nvals, vecs = np.linalg.eigh(Ham)\nvals[0:4]\n\narray([0.50042652, 1.49199271, 2.54377448, 3.30864454])\n\n\n\nplt.plot(xs,vecs[:,0:3])\n\n\n\n\nLet’s look at the accuracy of our solutions as a function of quantum number.\n\nplt.figure()\n\nNlist = [50,60,75,100]\nfor i in range(len(Nlist)):\n    dx = 20/Nlist[i]\n    xs = np.linspace(-10,10,Nlist[i])\n    Ham = make_H(xs,Vharmonic)\n    vals, vecs = np.linalg.eigh(Ham)\n    energies_exact = 0.5 + np.arange(0,len(xs),1)\n    errors = np.abs(vals - energies_exact)/energies_exact\n    plt.semilogy(np.arange(0,50,1),errors[0:50])\n    \nplt.legend([Nlist[i] for i in range(len(Nlist))])\nplt.xlabel(\"Quantum number, n\")\nplt.ylabel(\"Fractional error\")\n\nText(0, 0.5, 'Fractional error')\n\n\n\n\n\nQuestion: Explain the differences between the case with 50 grid points and the case with 60 grid points. Why is the agreement better to higher \\(n\\) for 60 grid points? Also, why does the agreement seem to always get worse above \\(n \\sim 25\\), even if the number of grid points is increased?\n\n2.4.1 Comparing DVR to Finite Differences\nLet’s look at the accuracy of the numerical solution as a function of the number of grid points. We can compare this to the results of HW 1 to see how the DVR method does against finite differences.\nIn this block, write a function that computes the fractional error in the SHO eigenstates as a function of the number of grid points used.\n\nNlist = np.logspace(1,2.5,10)\nerrorlist = np.zeros(len(Nlist))\nfor i in range(len(Nlist)):\n    N = Nlist[i]\n    xs = np.arange(-10,10,20/Nlist[i])\n    H = make_H(xs,Vharmonic)\n    evals, evecs = np.linalg.eigh(H)\n    errorlist[i] = np.abs(evals[0] - 0.5)/0.5\n\nThis next block is already written for you– nothing you need to do!— to implement and solve the same problem using the Numerov method. The code is taken from the HW1 solutions.\n\n# Function to make the A matrix:\ndef makeA(N,h):\n    A = np.zeros((N,N))\n    for i in range(N):\n        A[i,i] = -2.0/h**2 # fill in diagonal\n        if i==0: # the first row looks like [-2,1,0,0,0...]/h^2\n            A[i,i+1] = 1/h**2\n        elif i==N-1: # the last row looks like [...,0,0,1,-2]/h^2\n            A[i,i-1] = 1/h**2\n        else: # all other rows look like [...0,1,-2,1,0,...]/h^2\n            A[i,i+1] = 1/h**2\n            A[i,i-1] = 1/h**2\n    return A\n\n# Function to make the B matrix:\ndef makeB(N):\n    B = np.zeros((N,N))\n    for i in range(N):\n        B[i,i] = 10.0/12 # fill in diagonal\n        if i==0:\n            B[i,i+1] = 1/12\n        elif i==N-1:\n            B[i,i-1] = 1/12\n        else:\n            B[i,i+1] = 1/12\n            B[i,i-1] = 1/12\n    return B\n\n# Function to make the Hamiltonian matrix on a grid of N points between (xmin,xmax), then find eigenvalues/eigenvectors.\n# The potential energy Vfunc is an arbitrary function that gets passed in and discretized inside of this function.\ndef solve(N,xmin,xmax,Vfunc):\n    x = np.linspace(xmin,xmax,N) # grid of x values\n    h = x[1]-x[0] # grid spacing\n\n    # make the A and B matrices\n    A = makeA(N,h)\n    B = makeB(N)\n    Tmat = -0.5 * np.linalg.inv(B) @ A # This is the matrix representing kinetic energy.\n\n    # put the potential energy onto a grid and then put that into a diagonal matrix\n    Vgrid = np.zeros(N)\n    for i in range(N):\n        Vgrid[i] = Vfunc(x[i])\n    Vmat = np.diag(Vgrid)\n\n    # Finally we get the whole Hamiltonian\n    Hmat = Tmat + Vmat\n\n    # Now diagonalize the Hamiltonian. \n    # eigh returns sorted values and vectors!\n    evals, evecs = np.linalg.eigh(Hmat)\n    \n    return evals, evecs\n\n# NOW ACTUALLY CALL THE NUMEROV METHOD\nvals_numerov, vecs_numerov = solve(1000, -10,10, Vharmonic)\nerrorlist_numerov = np.zeros(len(Nlist))\nfor i in range(len(Nlist)):\n    N = round(Nlist[i])\n    vals_num, vecs_num = solve(N,-10,10,Vharmonic)\n    errorlist_numerov[i] = np.abs(vals_num[0] - 0.5)/0.5\n\nNow, make a plot of the relative errors comparing the numerov and DVR methods for the ground state as a function of the grid spacing \\(\\Delta x\\).\n\nplt.loglog([20/Nlist[i] for i in range(len(Nlist))],errorlist, label=\"DVR method\")\nplt.loglog([20/Nlist[i] for i in range(len(Nlist))],errorlist_numerov, 'r', label=\"Numerov method\")\nplt.legend()\nplt.xlabel(\"dx\")\nplt.ylabel(\"Fractional error\")\n\nText(0, 0.5, 'Fractional error')\n\n\n\n\n\nClearly the DVR method is much better than the Numerov technique! For reasonable grid sizes, the DVR method gets the ground-state energy with almost 10 orders of magnitude better relative agreement!! (And remember, the Numerov method was already much better than a “plain” second-order finite differences approximation to the Hamiltonian…)\nQuestion: Why do you think the DVR method stops getting more accurate at a certain step size? What does this tell you about setting up simulations?"
  },
  {
    "objectID": "Section6.html",
    "href": "Section6.html",
    "title": "3  Section 6: An Intuitive Picture of Band Structure",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider\nfrom IPython.display import display\nIn this Section, we are going to explore the emergence of band structure using simple models that you have studied before. We will consider a series of potential barriers (equally spaced) to simulate the periodic potential that electrons experience inside a crystal. By varying the heights of these barriers, we can smoothly transition between two simple limits: (1) a single, wide square well and (2) a series of narrow, uncoupled square wells. Band structure emerges between these two extremes.\nLearning Goals After this Section, you should: - Be able to explain the qualitative origins of energy bands - Explain the origin of energy spacings within bands as compared to between bands - Describe the shape of electronic wavefunctions at the edge of each Brillouin zone - Explain the connection between bound state and scattering methods of characterizing the energy levels of the periodic potential"
  },
  {
    "objectID": "Section6.html#qualitative-solution",
    "href": "Section6.html#qualitative-solution",
    "title": "3  Section 6: An Intuitive Picture of Band Structure",
    "section": "3.1 Qualitative Solution",
    "text": "3.1 Qualitative Solution\nBefore we dive into numerical solutions, let’s think about the following qualitative situation: we have a square well of width \\(L\\). Within the well, we place a series of barriers that partition the square well into \\(N\\) narrower wells with width \\(a\\). Each barrier has height \\(\\beta\\) and thickness \\(b\\).\nLet’s start by considering the barriers to be infinitely thin (basically, delta functions). Now, as a class, let’s work through the following questions:\n\nWhat are the energies and eigenstates of the wide well if the barriers are not present (\\(\\beta=0\\))?\nWhen \\(\\beta \\rightarrow \\infty\\), what should the wavefunctions look like?\nAs we increase \\(\\beta\\) to take on finite values, what will happen to the wavefunctions? How will they look? What will their ordering be like relative to the cases we just described?\nWhat analogies could we make between excitation of different bands and excitations of atoms in a solid?\n\nAfter going through these qualitative arguments, we can now look at what the numerics tell us."
  },
  {
    "objectID": "Section6.html#building-the-potential-vx",
    "href": "Section6.html#building-the-potential-vx",
    "title": "3  Section 6: An Intuitive Picture of Band Structure",
    "section": "3.2 Building the potential V(x)",
    "text": "3.2 Building the potential V(x)\nFirst we’ll write some functions that build our model potential. Our model includes the following parameters:\n\n\\(L\\): width of the enclosing square well\n\\(a\\): width of each narrow square well\n\\(b\\): thickness of each internal barrier\n\\(N\\): number of narrow square wells\n\\(\\beta\\): heigh of each barrier.\n\n\n\nText(0, 0.5, 'V')"
  },
  {
    "objectID": "Section6.html#making-the-dvr-code",
    "href": "Section6.html#making-the-dvr-code",
    "title": "3  Section 6: An Intuitive Picture of Band Structure",
    "section": "3.3 Making the DVR code",
    "text": "3.3 Making the DVR code\nNext, we set up the DVR code from Section 4 to build the Hamiltonian for our system. This code is the same as we have used previously.\n\nhbar = (6.63e-34)/(2*np.pi) # in J.s\nm = 9.1e-31 # in kg\n\n# Function to make the kinetic energy operator\ndef make_T(x):\n    Delta_x = x[1]-x[0]\n    N = x.shape[0]\n    Tmat = np.zeros((N,N))\n    \n    # now loop over kinetic energy matrix and fill in matrix elements\n    for i in range(N):\n        for j in range(N):\n            if i==j:\n                Tmat[i,j] = (hbar**2/(2*m*Delta_x**2)) * (np.pi**2)/3\n            else:\n                Tmat[i,j] = (hbar**2/(2*m*Delta_x**2)) * (-1)**(i-j) * 2/(i-j)**2\n                \n    return Tmat\n  \n# Function to make the potential energy operator\ndef make_V(x,Vfunc):\n    Vmat = np.diag(Vfunc(x))\n    return Vmat\n\n# Function to make the full Hamiltonian\ndef make_H(x,Vfunc):\n    return make_T(x) + make_V(x,Vfunc)\n\nLet’s test this on a potential consisting of 12 wells that are fairly deep. After building the potential, we construct the Hamiltonian, evaluate the eigenvectors/eigenvalues, and then plot.\n\nN = 12 # number of wells\na = 0.4e-9\nb = 0.05e-9 # wall thickness in nm\nbeta = 20 / 6.24150907e18 # well height in J\nV_wells = make_potential(N, a, b, beta)\nxs = np.linspace(-5, 5, 1600)*1e-9\nplt.plot(xs*1e9, V_wells(xs))\nplt.xlabel(\"Position (nm)\")\nplt.ylabel(\"Energy (eV)\")\n\nHam=make_H(xs,V_wells)\n\nvals, vecs = np.linalg.eigh(Ham)\n# make sure ground state is upward going\nvecs[:,0] = vecs[:,0] * np.sign(vecs[np.argmax(np.abs(vecs[:,0])),0])\n# number of bound states:\nnbound = np.sum([vals<beta])\n\n\n\n\nWe can now plot the numerically computed eigenvalues. Let’s overlay these values with the energy levels that would occur if there were no barriers inside the potential—the energy levels of the “wide” finite square well.\n\ndef analytical_Es(w):\n    return [i**2 * np.pi**2 * hbar**2/(2*m*w**2) for i in range(1,nbound+1)]\n\n\nplt.plot(vals[:nbound],'.')\nplt.plot(analytical_Es(N*a+(N-2)*b))\n\n\n\n\nNotice that the periodic potential leads to a structure highly reminiscient of band structure: the energy levels roughly follow the parabolic shape expected from the wide square well (or, from a different perspective, for the free particle), but the levels are broken up into bands with the characteristic shape that we see in class. And there are band gaps between these bands.\nQuestion: What will the plot look like if we increase the well separation to the point that the wells are essentially fully uncoupled? Will there be energy degeneracies? How many levels will share the same energy? What happens as the number of wells is increased–which energy gaps “close” and which remain?\nTo answer these questions, perform a “numerical experiment” and compare your results to the appropriate square well. How do the numbers compare?\nQuestion: What happens if the well separation is decreased to essentially zero? How do these energies compare to the appropriate finite square well? What about the comparison to free particle energies?\nLet’s look at the wavefunctions to understand this even better. It will be useful to plot the actual wavefunctions and have some interactivity. The code below makes a plot with a slider that allows you to look at all of the bound eigenstates for a given potential. Once you set the number of wells, their width, and their barriers, you can scan through the eigenstates and look at the behavior.\n\ndef interact_wavefunction():        \n    ## Plot parameters\n    xmin, xmax, nx = min(xs)*1e9, max(xs)*1e9, 50\n    ymin, ymax = -1.2, 1.2\n    pmin, pmax, pstep, pinit = 0, nbound-1, 1, 0\n    \n    ## Set up the plot data\n    fig, (ax1,ax2)   = plt.subplots(1,2,figsize=(16,6))\n    line2, = ax1.plot(xs*1e9,2*V_wells(xs)/np.max(V_wells(xs))-1, color=\"grey\")\n    line, = ax1.plot([], [], linewidth=2, color=\"red\") # Initialize curve to empty data.\n    \n    well_levels, = ax2.plot([],[])\n    well_itself, = ax2.plot([],[], color=\"black\")\n\n    ## Set up the figure axes, etc.\n    ax1.set_xlim([xmin, xmax])\n    ax1.set_ylim([ymin, ymax])\n    ax2.set_xlim([xmin,xmax])\n    ax2.set_ylim([-beta/8*6.24150907e18,1.2*beta*6.24150907e18])\n    ax1.set_xlabel('Position (nm)', fontsize=18)\n    ax1.set_ylabel('Re{$\\psi$}', fontsize=18)\n    ax2.set_xlabel('Position (nm)', fontsize=18)\n    ax2.set_ylabel('Energy (eV)', fontsize=18)\n    ax1.tick_params(axis='both', which='major', labelsize=16)\n    ax2.tick_params(axis='both', which='major', labelsize=16)\n    plt.close()      # Don't show the figure yet.\n\n    ## Callback function\n    def plot_wavefunction(Eigenstate):\n        y = vecs[:,int(Eigenstate)]/np.max(np.abs(vecs[:,int(Eigenstate)]))\n        line.set_data(xs*1e9, y)\n        line2.set_data(xs*1e9, 2*V_wells(xs)/np.max(V_wells(xs))-1)\n        well_itself.set_data(xs*1e9, V_wells(xs)*6.24150907e18)\n        [ax2.axhline(y=vals[i]*6.24150907e18,color=\"grey\") for i in range(nbound)]\n        ax2.axhline(y=vals[int(Eigenstate)]*6.24150907e18, color=\"cyan\")\n        ax2.set_title(f\"E={np.round(vals[int(Eigenstate)]*6.24150907e18,3)} eV\", fontsize=16)\n        display(fig)\n\n    ## Generate the user interface.\n    interact(plot_wavefunction, \n             Eigenstate=FloatSlider(min=pmin, max=pmax, step=pstep, value=pinit))\n\nFor example, we can start with 12 wells that are each 0.8 nm wide, 4 eV high, and have a 0.05 nm barrier between them. For this set of parameters, use the interactive plotting to gain some insights about the potential.\n\nN = 12 # number of wells\na = 0.8e-9\nb = 0.05e-9 # wall thickness in nm\nbeta = 4 / 6.24150907e18 # well height in J\nV_wells = make_potential(N, a, b, beta)\nxs = np.linspace(-8, 8, 900)*1e-9\n\nHam=make_H(xs,V_wells)\n\nvals, vecs = np.linalg.eigh(Ham)\n# make sure ground state is upward going\nvecs[:,0] = vecs[:,0] * np.sign(vecs[np.argmax(np.abs(vecs[:,0])),0])\n# number of bound states:\nnbound = np.sum([vals<beta])\n\n\ninteract_wavefunction();\n\n\n\n\nQuestion: Using the interactive graphs above, how does the behavior of each wavefunction within a single band conform to our expectations from Bloch’s theorem? How do the wavefunctions differ from band to band? What do you expect to happen as \\(N \\rightarrow \\infty\\)?"
  },
  {
    "objectID": "Section6.html#transfer-matrix-approach",
    "href": "Section6.html#transfer-matrix-approach",
    "title": "3  Section 6: An Intuitive Picture of Band Structure",
    "section": "3.4 Transfer matrix approach",
    "text": "3.4 Transfer matrix approach\nThere’s a totally different way to look at band structure: by studying the scattering properties of the potential. This is a complementary method to learn about a potentials bound states. Scattering studies are widely used in many fields of physics/chemistry, from studying the solid state to studying atoms and molecules. Let’s see what we can learn about electronic band structure from this point of view using the transfer matrix program that you wrote on HW2.\nFirst of all, let’s go over the idea of the transfer matrix approach.\nSet up approach, talk about different role of propagation vs. transmission matrices.\nThe code to implement each of these matrices is copied below.\n\ndef DMat(k1, k2):\n    res = np.zeros((2,2),dtype=np.complex_)\n    res[0,0] = (1 + k2/k1)/2\n    res[0,1] = (1 - k2/k1)/2\n    res[1,0] = res[0,1]\n    res[1,1] = res[0,0]\n    return res\n\ndef PMat(k, L):\n    res = np.zeros((2,2),dtype=np.complex_)\n    res[0,0] = np.exp(-1j * k * L)\n    res[1,1] = np.exp(1j * k * L)\n    return res\n\nQuestion: What do the peaks here represent? What is the physics going on when N=2? N=3? Large N?\n\na = 0.8e-9\nb = 0.1e-9\n\nEs = np.arange(0.01, 12.0, 0.001) / 6.24150907e18\nwidth_barrier = b\nwidth_gap = a\nVb=10/ 6.24150907e18\n\nNwells=30\n\n\nTtrans = np.zeros(Es.size)\ni = 0\nfor E in Es:\n    klow = np.emath.sqrt(2 * m * E/hbar**2)\n    khigh = np.emath.sqrt(2 * m * (E - Vb)/hbar**2)\n    res_mat = DMat(klow, khigh) @ PMat(khigh, width_barrier) @ DMat(khigh, klow) @ PMat(klow, width_gap)\n    U, V = np.linalg.eig(res_mat)\n    diag_res_mat = np.diag([U[0],U[1]])\n    res_mat = np.linalg.matrix_power(diag_res_mat,Nwells)\n    res_mat = V @ res_mat @ np.linalg.inv(V)\n    Ttrans[i] = 1 - np.abs(res_mat[1, 0])**2 / np.abs(res_mat[0,0])**2\n    i = i + 1\n\nplt.plot(Es * 6.24150907e18, Ttrans)\nplt.xlabel(\"Energy (eV)\",fontsize=16)\nplt.ylabel(\"Transmission\",fontsize=16)\nplt.tick_params(axis='both', which='major', labelsize=16)\n\n\n\n\nLet’s compare the transmission peaks to the bound state energy of our finite square well.\n\nV_wells = make_potential(Nwells, a, b, Vb)\nxs = np.linspace(-15, 15, 1500)*1e-9\n\nHam=make_H(xs,V_wells)\n\nvals, vecs = np.linalg.eigh(Ham)\n\nNow let’s compare the transmission fraction to the band energies.\nQuestion: How can we easily determine the limits of each energy band? (How does it compare to the number of wells being simulated?\n\nband_limits = [0]\nfor n in range(5):\n    band_limits.append(n*Nwells-1)\n    band_limits.append(n*Nwells)\n\nplt.plot(Es * 6.24150907e18, Ttrans, color=\"limegreen\")\n[plt.axvline(vals[i]*6.24150907e18, color=\"black\") for i in band_limits]\nplt.xlabel(\"Energy (eV)\",fontsize=16)\nplt.ylabel(\"Transmission\",fontsize=16)\nplt.tick_params(axis='both', which='major', labelsize=16)\nplt.xlim(0,10)\n\n(0.0, 10.0)"
  }
]